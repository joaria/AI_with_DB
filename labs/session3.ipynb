{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7308df8f",
   "metadata": {},
   "source": [
    "### Semantic Kernel Introduction\n",
    "\n",
    "#### Overview of Semantic Kernel (SK) and Its Importance\n",
    "***Semantic Kernel*** is an open-source SDK from Microsoft that acts as middleware between your application code and AI large language models (LLMs). It enables developers to easily integrate AI into apps by letting AI agents call code functions and by orchestrating complex tasks. SK is lightweight and modular, designed for enterprise-grade solutions with features like telemetry and filters for responsible AI. Major companies (including Microsoft) leverage SK because it’s flexible and future-proof – you can swap in new AI models as they emerge without rewriting your code. In short, SK helps build robust, scalable AI applications that can evolve with advancing AI capabilities.\n",
    "\n",
    "Key reasons why Semantic Kernel is important for AI application development:\n",
    "\n",
    "- Bridging AI and Code: **SK combines natural language prompts with your existing code and APIs**, allowing AI to take actions. The AI can request a function call and SK will execute that function and return results back to the model. This bridges the gap between what the AI intends and what your code can do.\n",
    "- **Plugins (Skills)**: You can expose functionalities (from simple math to complex business logic or external APIs) as SK plugins. By describing your code to the AI (via function definitions), the model can invoke these functions to fulfill user requests. This plugin architecture makes your AI solutions modular and extensible.\n",
    "- **Enterprise-ready**: SK includes support for security, observability, and compliance (e.g. integration with Azure services, monitoring, content filtering). Hooks and filters ensure you can enforce policies (for instance, prevent sensitive data leakage).\n",
    "- **Multi-modal & Future-Proof**: SK natively supports multiple AI services (OpenAI, Azure OpenAI, HuggingFace, etc.) and modalities. Chat-based APIs can be extended to voice or other modes. As new models (like vision-enabled models or better language models) come out, SK lets you plug them in without major changes.\n",
    "- **Rapid Development**: By handling the heavy lifting of prompt orchestration, function calling, and memory management, SK enables faster development of AI features. You focus on defining what you want the AI to do (skills, prompts) and SK handles how to do it. Microsoft claims that SK helps “deliver AI solutions faster than any other SDK” due to its ability to automatically call functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f19c8",
   "metadata": {},
   "source": [
    "### Services and Core Components of SK\n",
    "Semantic Kernel's architecture revolves around a few core components and services:\n",
    "\n",
    "- **Kernel**: The central object that orchestrates everything. The Kernel holds configuration for AI services, manages plugins (skills), coordinates function calls, and maintains contextual state (memory). You typically create one Kernel instance in your app and use it to register functions and perform AI queries.\n",
    "- **AI Services**: SK connects to AI models for different tasks:\n",
    "Chat Models: e.g. Azure OpenAI GPT-4o-mini or GPT-4o for natural language generation and understanding.\n",
    "Embedding Models: for converting text to vector embeddings (used in memory/search).\n",
    "Other Modalities: connectors for images, speech, etc., if needed.\n",
    "\n",
    "Semantic Kernel can automatically read your .env (automatically read from root of project) to access Azure OpenAI using the following variables:\n",
    "\n",
    "    AZURE_OPENAI_ENDPOINT - The endpoint it should talk to by default\n",
    "    AZURE_OPENAI_API_KEY - The API Key it should use\n",
    "    AZURE_OPENAI_API_VERSION - Inference API version it should use per default\n",
    "    AZURE_OPENAI_CHAT_DEPLOYMENT_NAME - Model deployment name it should use per default\n",
    "\n",
    "Before you continue, make sure your .env (copied from .env.sample) is filled out correctly.\n",
    "\n",
    "You configure the Kernel with the endpoints/keys for the services you need. For example, adding an Azure OpenAI chat completion service as follows ->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562d7aa",
   "metadata": {},
   "source": [
    "#### 1. Create a kernel and add a chat completion model to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f940329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion=AzureChatCompletion()\n",
    "\n",
    "kernel.add_service(chat_completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250a28d",
   "metadata": {},
   "source": [
    "### 2. Provide the kernel with the required skills. These can be defined as \"functions\" and \"plugins\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a4b5",
   "metadata": {},
   "source": [
    "##### Define function via a prompt (natural language function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8188f41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel es una caja de herramientas abierta y ligera que te ayuda a crear amiguitos inteligentes y poner modelos de inteligencia artificial super modernos en tus programas de manera fácil.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"{{$input}}\\n\\ntranslate to {{$target_language}} and change the tone to {{$target_tone}}:\"\n",
    "\n",
    "rewrite_expert = kernel.add_function(\n",
    "    prompt=prompt_template,\n",
    "    function_name=\"toner\",\n",
    "    plugin_name=\"Toner\"\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "sample_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your codebase\n",
    "\"\"\"\n",
    "\n",
    "# async function call need to be awaited\n",
    "summary = await kernel.invoke(rewrite_expert, input=sample_text, target_language = \"spanish\", target_tone=\"kids\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0ed94",
   "metadata": {},
   "source": [
    "#### Define function via code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401931bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions import kernel_function\n",
    "# Define the LightModel and LightsPlugin classes\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: int | None\n",
    "    hex: str | None\n",
    "\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self.lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> List[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"]\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Gets the state of a particular light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"], new_state: LightModel\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c916ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True), KernelParameterMetadata(name='new_state', description=None, default_value=None, type_='LightModel', is_required=True, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001506188AA50>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001506188AB10>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x000001506187E0D0>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state.', parameters=[], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='list[LightModel]', is_required=True, type_object=<class 'list'>, schema_data={'type': 'array'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001506188AC10>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001506188ACD0>, method=<bound method LightsPlugin.get_lights of <__main__.LightsPlugin object at 0x000001506187E0D0>>, stream_method=None), 'get_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_state', plugin_name='Lights', description='Gets the state of a particular light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001506188B190>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001506188B250>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x000001506187E0D0>>, stream_method=None)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies for the plugin\n",
    "# Example with toy data in memory\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "kernel = Kernel()\n",
    "kernel.add_plugin(\n",
    "    plugin=plugin,\n",
    "    plugin_name=\"Lights\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17d2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The table lamp is now turned on. If you meant a different lamp, please specify its name!\n",
      "Assistant > The table lamp is already on. If you want to adjust its brightness or color, let me know!\n",
      "Assistant > The table lamp is now turned off. Let me know if you need anything else!\n",
      "Assistant > The porch light is already off. If you'd like to turn it on or make other changes, just let me know!\n",
      "Assistant > The porch light is already off. If you need to turn on the porch light or adjust any settings, let me know!\n",
      "Assistant > All the lamps are now turned off. Here is the final state of all the lights:\n",
      "\n",
      "1. Table Lamp: Off, Brightness 100%, Color FF0000 (red)\n",
      "2. Porch light: Off, Brightness 50%, Color 00FF00 (green)\n",
      "3. Chandelier: Off, Brightness 75%, Color 0000FF (blue)\n",
      "\n",
      "Let me know if you need anything else!\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings)\n",
    "import logging\n",
    "\n",
    "# Enable  logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "user_message = \"Please turn on the lamp\"\n",
    "history.add_user_message(user_message)\n",
    "\n",
    "while(True):\n",
    "    # Get the response from the AI\n",
    "    \n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + str(result))\n",
    "    if(user_message.lower() == \"exit\"):\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    # Add the message from the agent to the chat history\n",
    "    history.add_message(result)\n",
    "\n",
    "    # Get user input for the next message\n",
    "    user_message = input(\"Type another request or say 'exit' to end the chat > \")\n",
    "    history.add_user_message(user_message)\n",
    "    if( user_message.lower() == \"exit\"):\n",
    "        #autiomatic function calling\n",
    "        history.add_user_message(\"Please turn off all the lamps and give me final state of all the lights\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533710c",
   "metadata": {},
   "source": [
    "### A real-world-like usecase: Netflix, Ltd. \n",
    "Let's assume that **Netflix, Ltd.** movie company, we want to be able to consult our DB for movies information.\n",
    "\n",
    "##### First, let's set up **the operational** database for Netflix, Ltd.\n",
    "- You need an Azure postgres db \n",
    "- Rename env.sample to .env and add credentials to .env file\n",
    "- You can run python db_inti.py to set up the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c95fe8",
   "metadata": {},
   "source": [
    "#### **Example 1**: an inventory analyst working at the sales departments, wants to perform below actions based on current inventory and sales data of the company:\n",
    "- Get the most updated inventory for a given product name or id\n",
    "- A list of most sold product names for a given categroy\n",
    "- Add a new product to the database\n",
    "\n",
    "This data is stored in a postgreSQL database.\n",
    "\n",
    "In this example, we take a simple approach as follows:\n",
    "1. **securely** Connect to the database\n",
    "2. Load data from required tables from the database when needed\n",
    "3. Provide all necessary functions and plugins\n",
    "4. Develop a code using semantic kernel to get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52883df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from src.get_conn import get_connection_uri\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "\n",
    "class Contoso_DataPlugin:\n",
    "    def __init__(self, db_uri: str):\n",
    "        self.conn = psycopg2.connect(db_uri)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        print(\"Connected to company's database successfully.\")\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_product_info(self, product_name: Optional[str] = None, product_id: Optional[int] = None) -> list[dict]:\n",
    "        \"\"\"Gets all movies information from the database given the name of the movie.\"\"\"\n",
    "        query = \"\"\"SELECT \n",
    "                    show_id,\n",
    "                    type,\n",
    "                    title,\n",
    "                    release_year,\n",
    "                    listed_in,\n",
    "                    description\n",
    "                    from netflix_shows\n",
    "                WHERE (LOWER(title) = LOWER(%(product_name)s) AND %(product_name)s IS NOT NULL)\n",
    "                   \"\"\"\n",
    "        if not product_name and not product_id:\n",
    "            print(\"No product name provided.\")\n",
    "            return None\n",
    "        elif product_id:\n",
    "            self.cursor.execute(query, {\"product_name\": None, \"product_id\": product_id})\n",
    "        else:\n",
    "            self.cursor.execute(query, {\"product_name\": product_name, \"product_id\": None})\n",
    "\n",
    "            \n",
    "        rows = self.cursor.fetchall()\n",
    "        columns = [desc[0] for desc in self.cursor.description]\n",
    "        try:\n",
    "            products= DataFrame(rows, columns=columns)\n",
    "            products.to_dict(orient=\"records\")  # <-- JSON serializabl\n",
    "            \n",
    "            return products.to_dict(orient=\"records\")  # <-- JSON serializabl\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching product information: {e}\")\n",
    "            return None\n",
    "    \n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fa594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection uri was rertieved successfully.\n"
     ]
    }
   ],
   "source": [
    "conn_uri = get_connection_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da043b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to company's database successfully.\n"
     ]
    }
   ],
   "source": [
    "chat_kernel = Kernel()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion = AzureChatCompletion()\n",
    "chat_kernel.add_service(chat_completion)\n",
    "\n",
    "contoso_plugin = Contoso_DataPlugin(db_uri=conn_uri)\n",
    "\n",
    "chat_kernel.add_plugin(\n",
    "    plugin = contoso_plugin,\n",
    "    plugin_name=\"contoso_plugin\"\n",
    ") \n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb94c22",
   "metadata": {},
   "source": [
    "#### Example 1: asking a question about a movie or show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7015921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > Here is the information I found for the requested shows:\n",
      "\n",
      "1. Stranger Things:\n",
      "   - Type: TV Show\n",
      "   - Release Year: 2019\n",
      "   - Genres: TV Horror, TV Mysteries, TV Sci-Fi & Fantasy\n",
      "   - Description: When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces, and one strange little girl.\n",
      "\n",
      "2. Wild Dog:\n",
      "   - Type: Movie\n",
      "   - Release Year: 2020\n",
      "   - Genres: Action & Adventure, International Movies\n",
      "   - Description: A brash but brilliant Indian intelligence agent leads a covert operation to nab the mastermind behind a series of attacks threatening national security.\n",
      "\n",
      "No information was found for the shows \"JHGD&*&^D\" or \"WILD DOGS.\" If you need more details or information about another show, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Please give me information about the show called WILD DOG\"\n",
    "history.add_user_message(user_message)\n",
    "\n",
    "# Get the response from the AI\n",
    "    \n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=chat_kernel,\n",
    ")\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0da4e7",
   "metadata": {},
   "source": [
    "#### Example 2: adding a new product record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 'Lenovo Thinkpad' added successfully.\n",
      "Assistant > The product \"Lenovo Thinkpad\" with the description \"Latest model,\" priced at $699.99, inventory of 5, not refurbished, and in the \"Laptops\" category, has been successfully added to the database. If you need more details or want to manage this product, let me know!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "user_message = \"add a new product with name 'Lenovo Thinkpad', description 'Latest model', price 699.99, inventory 5, refurbished False, category 'Laptops'\"\n",
    "history.add_user_message(user_message)\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=chat_kernel,\n",
    ")\n",
    "print(\"Assistant > \" + str(result))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c8932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
